# Configuração Otimizada para LLM Local - Fernando AI
# Use este arquivo se seu modelo está muito lento (>30s por resposta)

API_ENDPOINT=http://localhost:1234/v1/chat/completions
MODEL_NAME=llama-3.1-8b-lexi-uncensored-v2

# Configurações do bot
ADMIN_NUMBER=seu_numero_aqui
DUPLICATE_TIMEOUT=5000
GROUP_RANDOM_CHANCE=0.02

# ⚡ OTIMIZAÇÕES PARA MODELOS LENTOS ⚡
# Reduzindo histórico para acelerar processamento
MAX_HISTORY_MESSAGES=10          # Menos histórico = menos tokens = mais rápido

# Configurações do LLM otimizadas
MAX_TOKENS=400                   # Respostas mais curtas = processamento mais rápido
TEMPERATURE=0.7                  # Menos criativo = mais rápido
TOP_P=0.9
REQUEST_TIMEOUT=120000          # 2 minutos de timeout (era 30s)
MAX_RETRIES=2                   # Menos tentativas para falhar mais rápido

# Rate limiting mais conservador para modelos lentos
MAX_REQUESTS_PER_MINUTE=5       # Menos requests por minuto
MAX_REQUESTS_PER_HOUR=25        # Menos requests por hora

# Backup
BACKUP_DIR=./backups
MAX_BACKUPS=5                   # Menos backups para economizar espaço

# Logs
LOG_LEVEL=info

# Configurações de memória
MEMORY_FILE=memoria.json
MEMORY_BACKUP_PATH=./backups

# DICAS PARA ACELERAR SEU MODELO:
# 
# 1. No LM Studio:
#    - Use GPU se disponível
#    - Reduza context length para 2048 ou 4096
#    - Desabilite streaming para este bot
#
# 2. Sistema:
#    - Feche outros programas pesados
#    - Use modelo menor se possível (7B em vez de 8B)
#    - Verifique se tem RAM suficiente
#
# 3. Configurações do bot:
#    - MAX_HISTORY_MESSAGES=5 (ainda menos histórico)
#    - MAX_TOKENS=200 (respostas bem curtas)
#    - GROUP_RANDOM_CHANCE=0.01 (responde menos em grupos)
