// src/lib/llm.js
import { config } from 'dotenv';
import { logger } from './logger.js';
import { performanceMonitor, measureTime } from './performance.js';
import { errorLogger } from './errorHandler.js';
import fs from 'fs';
import { fileURLToPath } from 'url';
import path from 'path';

// Caminho para o arquivo .env
const __dirname = path.dirname(fileURLToPath(import.meta.url));
const envPath = path.resolve(__dirname, '../../.env');

// Configura√ß√µes din√¢micas que ser√£o atualizadas automaticamente
let llmConfig = {};

// Fun√ß√£o para carregar configura√ß√µes do .env
function loadEnvConfig() {
  try {
    // Recarrega o .env
    config({ path: envPath });
    
    const newConfig = {
      ENDPOINT: process.env.API_ENDPOINT,
      MODEL: process.env.MODEL_NAME,
      MAX_TOKENS: parseInt(process.env.MAX_TOKENS) || 800,
      TEMPERATURE: parseFloat(process.env.TEMPERATURE) || 0.75,
      TOP_P: parseFloat(process.env.TOP_P) || 0.9,
      REQUEST_TIMEOUT: parseInt(process.env.REQUEST_TIMEOUT) || 120000,
      MAX_RETRIES: parseInt(process.env.MAX_RETRIES) || 3
    };

    // Valida√ß√£o das configura√ß√µes
    if (!newConfig.ENDPOINT) {
      logger.error('ERRO: API_ENDPOINT n√£o configurado no .env');
      return false;
    }

    if (!newConfig.MODEL) {
      logger.error('ERRO: MODEL_NAME n√£o configurado no .env');
      return false;
    }

    // Verifica se houve mudan√ßas
    const configChanged = JSON.stringify(llmConfig) !== JSON.stringify(newConfig);
    
    if (configChanged && Object.keys(llmConfig).length > 0) {
      logger.info('üîÑ Configura√ß√µes do LLM atualizadas automaticamente');
      logger.info('üìù Novo modelo: %s', newConfig.MODEL);
      logger.info('üåê Novo endpoint: %s', newConfig.ENDPOINT);
    }

    llmConfig = newConfig;
    return true;
  } catch (error) {
    logger.error('Erro ao carregar configura√ß√µes do .env:', error);
    return false;
  }
}

// Carrega configura√ß√µes iniciais
loadEnvConfig();
logger.info('LLM configurado: %s via %s', llmConfig.MODEL, llmConfig.ENDPOINT);

// Monitora mudan√ßas no arquivo .env
if (fs.existsSync(envPath)) {
  fs.watchFile(envPath, { interval: 1000 }, (curr, prev) => {
    if (curr.mtime > prev.mtime) {
      logger.info('üìÅ Arquivo .env alterado, recarregando configura√ß√µes...');
      loadEnvConfig();
    }
  });
  logger.info('üëÄ Monitoramento autom√°tico do .env ativado');
} else {
  logger.warn('‚ö†Ô∏è Arquivo .env n√£o encontrado para monitoramento');
}

/**
 * Cria um controller para timeout de requisi√ß√µes
 */
function createTimeoutController(timeoutMs) {
  const controller = new AbortController();
  const timeout = setTimeout(() => {
    controller.abort();
  }, timeoutMs);
  
  return { controller, timeout };
}

/**
 * Valida a resposta da API
 */
function validateResponse(data) {
  if (!data) {
    throw new Error('Resposta vazia da API');
  }
  
  if (data.error) {
    throw new Error(`Erro da API: ${data.error.message || data.error}`);
  }
  
  if (!data.choices || !Array.isArray(data.choices) || data.choices.length === 0) {
    throw new Error('Formato de resposta inv√°lido: sem choices');
  }
  
  const choice = data.choices[0];
  if (!choice.message || !choice.message.content) {
    throw new Error('Formato de resposta inv√°lido: sem conte√∫do');
  }
  
  return choice.message.content.trim();
}

/**
 * Prepara as mensagens para envio
 */
function prepareMessages(messages) {
  if (!Array.isArray(messages)) {
    throw new Error('Messages deve ser um array');
  }
  
  return messages.map(msg => ({
    role: msg.role,
    content: msg.content.slice(0, 2000) // Limita tamanho para acelerar processamento
  }));
}

/**
 * Calcula delay exponencial para retry
 */
function getRetryDelay(attempt) {
  return Math.min(1000 * Math.pow(2, attempt - 1), 10000); // Max 10s
}

/**
 * Faz requisi√ß√£o para o modelo LLM com retry robusto
 */
export const askLLM = measureTime(async function(messages) {
  if (!messages || messages.length === 0) {
    throw new Error('Mensagens n√£o podem estar vazias');
  }

  const preparedMessages = prepareMessages(messages);
  
  for (let attempt = 1; attempt <= llmConfig.MAX_RETRIES; attempt++) {
    const { controller, timeout } = createTimeoutController(llmConfig.REQUEST_TIMEOUT);
    
    try {
      logger.info('BOT: Enviando para LLM (tentativa %d/%d) - Aguarde, modelo pode demorar...', attempt, llmConfig.MAX_RETRIES);
      
      const requestBody = {
        model: llmConfig.MODEL,
        messages: preparedMessages,
        max_tokens: llmConfig.MAX_TOKENS,
        temperature: llmConfig.TEMPERATURE,
        top_p: llmConfig.TOP_P,
        stream: false // Garante resposta √∫nica
      };

      const response = await fetch(llmConfig.ENDPOINT, {
        method: 'POST',
        headers: { 
          'Content-Type': 'application/json',
          'User-Agent': 'FernandoAI/1.0'
        },
        body: JSON.stringify(requestBody),
        signal: controller.signal
      });

      clearTimeout(timeout);

      if (!response.ok) {
        const errorText = await response.text().catch(() => 'Erro desconhecido');
        throw new Error(`HTTP ${response.status}: ${errorText}`);
      }

      const data = await response.json();
      const content = validateResponse(data);
      
      logger.debug('‚úÖ Resposta recebida do LLM (%d chars)', content.length);
      
      // Log de uso de tokens se dispon√≠vel
      if (data.usage) {
        logger.debug('üìä Tokens: %d prompt + %d completion = %d total', 
                    data.usage.prompt_tokens || 0,
                    data.usage.completion_tokens || 0,
                    data.usage.total_tokens || 0);
      }
      
      return content;

    } catch (error) {
      clearTimeout(timeout);
      
      const isLastAttempt = attempt === llmConfig.MAX_RETRIES;
      const isTimeoutError = error.name === 'AbortError';
      const isNetworkError = error.message.includes('fetch') || isTimeoutError;
      
      logger.warn('‚ö†Ô∏è Tentativa %d falhou: %s', attempt, error.message);

      if (isLastAttempt) {
        await errorLogger.logError(error, {
          context: 'llm_request_failed',
          endpoint: ENDPOINT,
          model: MODEL,
          attempt: attempt,
          maxRetries: MAX_RETRIES,
          messageCount: preparedMessages.length,
          isTimeoutError,
          isNetworkError
        });
        
        logger.error('ERRO: Todas as tentativas falharam para LLM');
        
        // Erro mais espec√≠fico para timeout
        if (isTimeoutError) {
          throw new Error(`Timeout ap√≥s ${REQUEST_TIMEOUT}ms - modelo pode estar sobrecarregado`);
        }
        
        throw new Error(`Falha na comunica√ß√£o com LLM: ${error.message}`);
      }

      // Apenas faz retry em erros de rede ou tempor√°rios
      if (isNetworkError || error.message.includes('500') || error.message.includes('502') || error.message.includes('503')) {
        const delay = getRetryDelay(attempt);
        logger.debug('‚è∞ Aguardando %dms antes da pr√≥xima tentativa...', delay);
        await new Promise(resolve => setTimeout(resolve, delay));
      } else {
        // Erros permanentes (400, 401, etc.) n√£o fazem retry
        throw error;
      }
    }
  }
});

/**
 * Testa a conex√£o com o LLM
 */
export async function testLLMConnection() {
  try {
    const testMessages = [
      { role: 'system', content: 'Voc√™ √© um assistente de teste.' },
      { role: 'user', content: 'Responda apenas "OK" se estiver funcionando.' }
    ];
    
    const response = await askLLM(testMessages);
    const isWorking = response.toLowerCase().includes('ok');
    
    logger.info('üß™ Teste de conex√£o LLM: %s', isWorking ? '‚úÖ OK' : '‚ö†Ô∏è Resposta inesperada');
    
    return {
      success: true,
      response: response,
      working: isWorking
    };
    
  } catch (error) {
    logger.error('ERRO: Teste de conex√£o LLM falhou: %s', error.message);
    
    return {
      success: false,
      error: error.message,
      working: false
    };
  }
}

/**
 * Obt√©m informa√ß√µes sobre o modelo
 */
export function getModelInfo() {
  return {
    endpoint: llmConfig.ENDPOINT,
    model: llmConfig.MODEL,
    maxTokens: llmConfig.MAX_TOKENS,
    temperature: llmConfig.TEMPERATURE,
    topP: llmConfig.TOP_P,
    timeout: llmConfig.REQUEST_TIMEOUT,
    maxRetries: llmConfig.MAX_RETRIES
  };
}
